{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da74432c-9a69-405c-b00a-d212e9f2a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pyphonetics\n",
    "import phonetics\n",
    "import pydata_google_auth\n",
    "import Monkey_Type_Detection as mtd\n",
    "from arcgis.gis import GIS\n",
    "from getpass import getpass\n",
    "import skimpy\n",
    "import wikipedia as wiki\n",
    "import requests\n",
    "from arcgis.geocoding import batch_geocode, Geocoder, get_geocoders, batch_geocode, geocode\n",
    "from google_trans_new import google_translator\n",
    "import detect_delimiter\n",
    "\n",
    "\n",
    "def shopify_data_preprocess(df):\n",
    "    #two address lines - \n",
    "    df.loc[df['shipping_address_address1'].isnull()==False,'shipping_address_address1']=df['shipping_address_address1'].str.lower()\n",
    "    df.loc[df['shipping_address_address2'].isnull()==False,'shipping_address_address2']=df['shipping_address_address2'].str.lower()\n",
    "    df.loc[df['shipping_address_address1'].isnull()==True,'shipping_address_address1']='' #set empty address\n",
    "    df.loc[df['shipping_address_address2'].isnull()==True,'shipping_address_address2']=''\n",
    "    #get overall address\n",
    "    df.loc[df['shipping_address_address1'].isnull()==False,'whole_shipping_address']=df['shipping_address_address1']+','+df['shipping_address_address2']\n",
    "\n",
    "    #check for invalid address - frequency\n",
    "    df.loc[df['shipping_address_city'].isnull()==False,'shipping_address_city']=df['shipping_address_city'].str.lower()\n",
    "    df.rename(columns={'whole_shipping_address':'address_string','shipping_address_city':'drop_location_city','shipping_address_zip':'drop_location_pincode'},inplace=True)\n",
    "    \n",
    "    address_short=df[['address_string','drop_location_city','drop_location_pincode']]\n",
    "    return address_short\n",
    "\n",
    "def woocom_data_preprocess(df):\n",
    "    woocom_data.loc[woocom_data['drop_location_address'].isnull()==False,'address_string']=woocom_data['drop_location_address'].str.lower()\n",
    "    woocom_data.loc[woocom_data['drop_location_address'].isnull()==True,'address_string']='' #if null set empty string\n",
    "    woocom_data.rename(columns={'drop_location_pin':'drop_location_pincode'},inplace=True)\n",
    "    address_short=df[['address_string','drop_location_city','drop_location_pincode']]\n",
    "    return address_short\n",
    "\n",
    "def shipments_data_preprocess(df):\n",
    "    #two address lines - \n",
    "    df.loc[df['drop_location_address_1'].isnull()==False,'drop_location_address_1']=df['drop_location_address_1'].str.lower()\n",
    "    df.loc[df['drop_location_address_2'].isnull()==False,'drop_location_address_1']=df['drop_location_address_1'].str.lower()\n",
    "    df.loc[df['drop_location_address_1'].isnull()==True,'drop_location_address_1']='' #set empty address\n",
    "    df.loc[df['drop_location_address_1'].isnull()==True,'drop_location_address_1']=''\n",
    "    #get overall address\n",
    "    df.loc[df['drop_location_address_1'].isnull()==False,'whole_shipping_address']=df['drop_location_address_1']+','+df['drop_location_address_2']\n",
    "    df.loc[df['drop_location_city'].isnull()==False,'drop_location_city']=df['drop_location_city'].str.lower()\n",
    "\n",
    "    #check for invalid address - frequency\n",
    "    df.rename(columns={'whole_shipping_address':'address_string'},inplace=True)\n",
    "    \n",
    "    address_short=df[['address_string','drop_location_city','drop_location_pincode']]\n",
    "    return address_short \n",
    "\n",
    "def osm_data_preprocess():\n",
    "    # key_freq_df=pd.read_csv('key_freq_df.csv')\n",
    "    #get the frequent tags - \n",
    "    #generate cols for all the addresses\n",
    "    # import osm_data_extraction\n",
    "    #read generated file - for OSM format data\n",
    "    osm_data=pd.read_parquet('open_street_map_data/overall_osm_df.pq')\n",
    "    #here name - address, \n",
    "    #location entity/tag - hierarchy of info/location attribute, highway, junction, building, amenity, nature of location , shop type\n",
    "    #lat long\n",
    "    osm_data.loc[osm_data['name'].isnull()==False,'name']=osm_data['name'].str.lower()\n",
    "    osm_data.loc[osm_data['name'].isnull()==True,'name']='' #set location name empty\n",
    "    #get building name - \n",
    "    osm_data.loc[osm_data['building'].isnull()==False,'building']=osm_data['building'].str.lower()\n",
    "    osm_data.loc[osm_data['building'].isnull()==True,'building']='' #set building name empty\n",
    "    #get city, district, postalcode, location - place - tag\n",
    "    osm_data.loc[osm_data['district'].isnull()==False,'district']=osm_data['district'].str.lower()\n",
    "    osm_data.loc[osm_data['district'].isnull()==True,'district']='' #set district name empty\n",
    "\n",
    "    osm_data.loc[osm_data['village'].isnull()==False,'village']=osm_data['village'].str.lower()\n",
    "    osm_data.loc[osm_data['village'].isnull()==True,'village']='' #set village name empty\n",
    "\n",
    "    osm_data.loc[osm_data['town'].isnull()==False,'town']=osm_data['town'].str.lower()\n",
    "    osm_data.loc[osm_data['town'].isnull()==True,'town']='' #set town name empty\n",
    "\n",
    "    osm_data.loc[osm_data['locality'].isnull()==False,'locality']=osm_data['locality'].str.lower()\n",
    "    osm_data.loc[osm_data['locality'].isnull()==True,'locality']='' #set locality name empty\n",
    "    \n",
    "    osm_data.loc[osm_data['city'].isnull()==False,'city']=osm_data['city'].str.lower()\n",
    "    osm_data.loc[osm_data['city'].isnull()==True,'city']='' #set city name empty\n",
    "\n",
    "    osm_data.loc[osm_data['postal_code'].isnull()==False,'postal_code']=osm_data['postal_code'].str.lower()\n",
    "    osm_data.loc[osm_data['postal_code'].isnull()==True,'postal_code']='' #set postal_code name empty\n",
    "    \n",
    "    osm_data.loc[osm_data['highway'].isnull()==False,'highway']=osm_data['highway'].str.lower()\n",
    "    osm_data.loc[osm_data['highway'].isnull()==True,'highway']='' #set highway name empty\n",
    "    \n",
    "    osm_data.loc[osm_data['amenity'].isnull()==False,'amenity']=osm_data['amenity'].str.lower()\n",
    "    osm_data.loc[osm_data['amenity'].isnull()==True,'amenity']='' #set amenity name empty\n",
    "    \n",
    "    osm_data.loc[osm_data['place'].isnull()==False,'place']=osm_data['place'].str.lower()\n",
    "    osm_data.loc[osm_data['place'].isnull()==True,'place']='' #set place name empty\n",
    "    \n",
    "    osm_data.loc[osm_data['state'].isnull()==False,'state']=osm_data['state'].str.lower()\n",
    "    osm_data.loc[osm_data['state'].isnull()==True,'state']='' #set state name empty\n",
    "    \n",
    "    osm_data.loc[osm_data['brand'].isnull()==False,'brand']=osm_data['brand'].str.lower()\n",
    "    osm_data.loc[osm_data['brand'].isnull()==True,'brand']='' #set brand name empty\n",
    "    \n",
    "    \n",
    "    osm_data.loc[osm_data['street'].isnull()==False,'street']=osm_data['street'].str.lower()\n",
    "    osm_data.loc[osm_data['street'].isnull()==True,'street']='' #set street name empty\n",
    "\n",
    "    osm_data.loc[osm_data['housenumber'].isnull()==False,'housenumber']=osm_data['housenumber'].astype(str)\n",
    "    \n",
    "    osm_data.loc[osm_data['housenumber'].isnull()==False,'housenumber']=osm_data['housenumber'].str.lower()\n",
    "    osm_data.loc[osm_data['housenumber'].isnull()==True,'housenumber']='' #set housenumber name empty\n",
    "    \n",
    "    \n",
    "\n",
    "    osm_data.loc[osm_data['subdistrict'].isnull()==False,'subdistrict']=osm_data['subdistrict'].str.lower()\n",
    "    osm_data.loc[osm_data['subdistrict'].isnull()==True,'subdistrict']='' #set subdistrict name empty\n",
    "\n",
    "    osm_data.loc[osm_data['population'].isnull()==False,'population']=osm_data['population'].astype(str)\n",
    "\n",
    "    #can be used as an input feature - , can be fetched from google maps API\n",
    "    \n",
    "    osm_data.loc[osm_data['population'].isnull()==False,'population']=osm_data['population'].str.lower()\n",
    "    osm_data.loc[osm_data['population'].isnull()==True,'population']='' #set population name empty\n",
    "\n",
    "    ##get postal code - from geocoding\n",
    "    import geopu\n",
    "\n",
    "    osm_data.rename(columns={''})\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4587a210-7e36-466b-9be1-eb8540e6066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##consolidate all data to one - use multiprocessing to load data from various sources\n",
    "open_street_address_db=pd.read_pickle('formatted_open_street_address_db.pkl')\n",
    "woocom_data=pd.read_pickle('woocom.pkl')\n",
    "shopify_data=pd.read_pickle('shopify_data.pkl')\n",
    "formatted_open_street_address_db=pd.read_pickle('formatted_open_street_address_db.pkl')\n",
    "dataset_promise_merge=pd.read_csv('dataset_promise_merge.csv')\n",
    "address_shipments_data=pd.read_pickle('address_shipments_data.pkl')\n",
    "\n",
    "shopify_data_copy=shopify_data.copy()\n",
    "woocom_data_copy=woocom_data.copy()\n",
    "address_shipments_data_copy=address_shipments_data.copy()\n",
    "\n",
    "shopify_data_preprocessed=shopify_data_preprocess(shopify_data)\n",
    "woocom_data_preprocessed=woocom_data_preprocess(woocom_data)\n",
    "shipments_data_preprocessed=shipments_data_preprocess(address_shipments_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5cb0299-894c-4d7a-a970-09f64d39c414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "engine must be one of 'pyarrow', 'fastparquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11168\\3387373183.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m315\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"i: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'osm_parsed_data_v2/osm_df_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.pq'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0moverall_osm_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moverall_osm_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dash_env\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m     \"\"\"\n\u001b[1;32m--> 491\u001b[1;33m     \u001b[0mimpl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m     return impl.read(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dash_env\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mget_engine\u001b[1;34m(engine)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mFastParquetImpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"engine must be one of 'pyarrow', 'fastparquet'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: engine must be one of 'pyarrow', 'fastparquet'"
     ]
    }
   ],
   "source": [
    "overall_osm_df=pd.DataFrame()\n",
    "for i in range(315):\n",
    "    print(\"i: \",i)\n",
    "    df=pd.read_parquet('osm_parsed_data_v2/osm_df_'+str(i+1)+'.pq')\n",
    "    overall_osm_df=pd.concat(overall_osm_df,df)\n",
    "\n",
    "org_df=pd.read_parquet('overall_osm_df.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "919758ba-9f29-492b-8cdb-996b47299c14",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shipments_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11168\\2980289668.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mshipments_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'shipments_data' is not defined"
     ]
    }
   ],
   "source": [
    "shipments_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9066d4b-a4b5-4045-a852-2f54ef60f85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi! my na-m:e is ni_k, we\tcome; to datagy\n",
      "good\n",
      "bad\n",
      "['hi', '', 'my', 'na', 'm', 'e', 'is', 'ni', 'k', '', 'we', 'come', '', 'to', 'datagy', 'good', 'bad']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " '',\n",
       " 'my',\n",
       " 'na',\n",
       " 'm',\n",
       " 'e',\n",
       " 'is',\n",
       " 'ni',\n",
       " 'k',\n",
       " '',\n",
       " 'we',\n",
       " 'come',\n",
       " '',\n",
       " 'to',\n",
       " 'datagy',\n",
       " 'good',\n",
       " 'bad']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_by_delimiters(text):\n",
    "    split_string = re.split(r',|!|;|-|:|_| |\\t|\\n', text)\n",
    "    print(text)\n",
    "    print(split_string)\n",
    "    return split_string #return lexical units - split by various delimiters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adde7bcf-c9f5-4b97-89f2-54e5862fde19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7729faec-41a3-40e1-be31-f9d03e48adbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_delim(word, text):\n",
    "    comma_text_split=text.split(',')\n",
    "    word_start_index=text.find(word)\n",
    "    word_end_index=word_start_index+len(word)\n",
    "    word_before_char=[]\n",
    "    word_before_text=\n",
    "    if word not in comma_text_split:\n",
    "        return 0\n",
    "    elif substr_before_text==text[(word_start_index-1)]:word_end_index]\n",
    "    \n",
    "    \n",
    "word_comma_freq=pd.DataFrame(columns={'word','comma_before_freq','comma_after_freq'})\n",
    "all_words_comma_freq_track=pd.DataFrame(columns={'word','comma_before_freq','comma_after_freq'})\n",
    "all_words_list=[]\n",
    "def insert_comma_by_frequency(text):\n",
    "    text_list=split_by_delimiters(text) #split by all the delimiters - to get individual words\n",
    "    for word in text_list:\n",
    "        if word not in all_words_list:\n",
    "            all_words_list.append(word) #adding the newly encountered word into large address word corpus\n",
    "            word_comma_freq['word']=word\n",
    "            word_comma_freq['comma_before_freq']=0\n",
    "            word_comma_freq['comma_after_freq']=0\n",
    "        if word in text_list:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900dd401-40a7-4092-beb2-ce172e134617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_ext(file_name):\n",
    "    file_ext=file_name[len(file_name)-5:].rpartition('.')[1]\n",
    "    return file_ext\n",
    "\n",
    "def read_file(file_name,file_ext):\n",
    "    #extract file name excluding the file extension -\n",
    "    #add other possible file formats for data source here.\n",
    "    file_name_without_ext=file_name.rpartition('.')[0]\n",
    "    df=pd.DataFrame()\n",
    "    if file_ext=='csv':\n",
    "        df=pd.read_csv(file_name)\n",
    "    elif file_ext=='pq':\n",
    "        df=pd.read_parquet(file_name)\n",
    "    elif file_ext=='pkl':\n",
    "        df=pd.read_pickle(file_name)\n",
    "    elif file_ext=='json':\n",
    "        df=pd.read_json(file_name)\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Invalid file format...\", file_ext)\n",
    "    return df\n",
    "\n",
    "def get_postal_pincode_data():\n",
    "    \n",
    "    from skimpy import clean_columns\n",
    "\n",
    "\n",
    "\n",
    "    postal_pincode_data=pd.read_csv('postal_pincode_data.csv')\n",
    "    messy_df = postal_pincode_data\n",
    "    clean_df = clean_columns(messy_df)\n",
    "    clean_df.columns.tolist()\n",
    "\n",
    "def regex_match(pincode,pattern):\n",
    "    if re.match(pincode, pattern):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def check_pincode_correct(pincode, address):\n",
    "    pincode=str(pincode) #convert the input pincode - into string format, if not already - the first digit in pincode is zero - it will be removed, and pincode - will be read as 5 digits\n",
    "    #check if it has 6 digits\n",
    "    if len(pincode)<6:\n",
    "        raise Exception(\"Invalid pincode - pincode contains less than 6 digits\")\n",
    "    elif len(pincode)>6:\n",
    "        raise Exception(\"Pincode cannot have more than 6 digits...\")\n",
    "    elif pincode[0]=='0':\n",
    "        raise Exception(\"First digit of pincode cannot be zero...\")\n",
    "\n",
    "\n",
    "    pincode_pattern=r\"^[1-9]{1}[0-9]{2}\\\\s{0,1}[0-9]{3}$\"\n",
    "    pincode_pattern_valid=regex_match(pincode_pattern,pincode)\n",
    "    if (pincode_pattern_valid==False):\n",
    "        raise Exception(\"Pincode Format not valid...\")\n",
    "    else: #entered pincode format is valid\n",
    "        #now check if the entered pincode present in promise or postal database -\n",
    "        postal_pincode_df=pd.read_csv('postal_pincode_data.csv')\n",
    "        #convert both postal pincode and promise pincode list - to str types - to append both lists\n",
    "        postal_pincode_df['pincode']=postal_pincode_df['pincode'].astype(str)\n",
    "        postal_pincode_list=list(postal_pincode_df['pincode'].drop_duplicates().to_numpy())\n",
    "        print(\"len of postal_pincode_list: \",len(postal_pincode_list))\n",
    "\n",
    "        promise_pincode_df=pd.read_csv('promise_pincode_data.csv')\n",
    "        promise_pincode_df['pincode']=promise_pincode_df['pincode'].astype(str)\n",
    "        promise_pincode_list=list(promise_pincode_df['pincode'].drop_duplicates().to_numpy())\n",
    "        print(\"len of promise_pincode_list: \",len(promise_pincode_list))\n",
    "        #get combined list of two pincodes -\n",
    "        pincode_list=list(set(postal_pincode_list)|set(promise_pincode_list))\n",
    "        print(\"len of pincode list: \",len(pincode_list))\n",
    "\n",
    "\n",
    "\n",
    "        #check if pincode present in the overall pincode list -\n",
    "        if pincode not in pincode_list:\n",
    "            raise Exception(\"Pincode not found in database...\")\n",
    "        else:\n",
    "            #now if the format of the entered pincode is valid and if it is also found in the overall pincode database...\n",
    "            #Next step is to see if the pincode matches - the entered address\n",
    "            #Two ways to check -\n",
    "            # if the city matches pincode\n",
    "            # if the locality or other smaller - division - mapped to a pincode - based on postal pincode data\n",
    "                        \"\"\"\n",
    "                        Postal data - attributes -\n",
    "                        officename\tpincode\tofficeType\tDeliverystatus\tdivisionname regionname\tcirclename\tTaluk\tDistrictname\tstatename\tTelephone\tRelated Suboffice\tRelated Headoffice\tlongitude\tlatitude\n",
    "\n",
    "                        \"\"\"\n",
    "        pincode_expected_address_component_df=postal_pincode_df.loc[postal_pincode_df['pincode']==pincode,['officename','officeType','divisionname','regionname','circlename','Taluk','Districtname','statename','Related Suboffice','Related Headoffice','longitude','latitude']]\n",
    "        postal_data_cols=['officename','officeType','divisionname','regionname','circlename','Taluk','Districtname','statename','Related Suboffice','Related Headoffice','longitude','latitude']\n",
    "        for col in postal_data_cols:\n",
    "            #convert dtype to str - for all the address_dataset part strings\n",
    "            pincode_expected_address_component_df.loc[pincode_expected_address_component_df[col].isnull()==True,col]=\"None\"\n",
    "            pincode_expected_address_component_df[col]=pincode_expected_address_component_df[col].astype(str)\n",
    "            #convert the address_dataset substr - to lower\n",
    "            pincode_expected_address_component_df[col]=pincode_expected_address_component_df[col].str.lower()\n",
    "            #get pincode's correspondong - address string - address\n",
    "            pincode_address_mismatch=pincode_address_match_similarity(pincode,address,pincode_expected_address_component_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pincode_address_match_similarity(pincode, address, expected_address):\n",
    "    #check for expected address' similarity with actual address given for a pincode -\n",
    "\n",
    "#to map address to the type - for eg. if an address corresponds to an apartment or individual building\n",
    "        \"\"\"\n",
    "        1. solution 1 - use Google Maps API to see if the extracted door number of a specific street name corresponds to an apartment or a house - use Selenium - to extract the address details - corresponding to the search - if we need to avoid a geolocation API\n",
    "\n",
    "        2. solution 2 - analyze the deliveries history for the same address (door number + street name) and if the total number of deliveries is significantly higher than the average, it should be an indicator that it refers to an apartment or office space, if it significantly lower it may point to it being a house\n",
    "        \"\"\"\n",
    "def set_typology_feature_for_address(address):\n",
    "    pass\n",
    "\n",
    "#to classify an address - residential or a commercial/industrial area\n",
    "\n",
    "\"\"\"\n",
    "1. infer from address corresponding details - using geolocation API/extract addition location info - using selenium from maps.google.com - used for tagging area type -\n",
    "2. analyze - the neigbouring addresses - if they are tagged - by area type and infer - area type - based on the nearest address info we have\n",
    "\"\"\"\n",
    "def set_area_feature_for_address(address):\n",
    "    pass\n",
    "\n",
    "def locality_set_for_given_pincode(pincode):\n",
    "    \"\"\"\n",
    "    refer to ACT db - to get the range of localities a pincode is mapped to - along with\n",
    "    only use delivered address examples without any address issue or pincode address mismatch issue identified - also\n",
    "    use - courier partner's lat long and address lat long - from shopify or geolocation api mapping (used for training data)\n",
    "    if the distance between the two lat long coordinates - less than a threshold -\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#clean data - remove missing, incorrect pincodes,duplicate pincodes\n",
    "#get pincode, taluk mapping\n",
    "def geolocation_mapping_fwd(x): #address to lat long mapping\n",
    "    #using geopy, using arcgis, using position stack\n",
    "    \"\"\" http://api.positionstack.com/v1/forward\n",
    "        ? access_key =\"\"\"\n",
    "    import requests\n",
    "    #try getting location data from google maps\n",
    "    response = requests.get('https://google.com/')\n",
    "    print(response)\n",
    "    \"\"\"<Response [200]>\n",
    "        & query = 1600 Pennsylvania Ave NW, Washington DC\"\"\"\n",
    "\n",
    "def geolocation_mapping_reverse(x): #lat long to address -> address entity mappingdef composite_function(f, g):\n",
    "        # return lambda x : f(g(x))\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "def pincode_validation(pincode):\n",
    "    pincode_format_valid=check_pincode_format(pincode)\n",
    "\n",
    "def address_raw_data(data_source_file):\n",
    "    #identify the type of file -\n",
    "    #get file extention -\n",
    "    file_ext=data_source_file.get_file_ext()\n",
    "    #get address data - from different sources -\n",
    "    # 1. Shipment History\n",
    "    # 2. Shopify and Woocom Orders data - both swift/non-swift fulfilled - shipping and billing address, pickup addresses\n",
    "    # 3. External Data Sources -\n",
    "    #   a. data gov - websites - indian address data sources (need additional sources - MapMyIndia,\n",
    "    #   b. OSM Indian Address data\n",
    "    #   c. Require - list of zone - ward, street level info - whenever available for a city\n",
    "    #   c. Amazon - lat long mapping for the delivered addresses - uses GPS Coordinates (?)\n",
    "    #   d. Delhivery - lat long mapping for the delivered addresses - uses GPS Coordinates\n",
    "\n",
    "    address_data=read_file_name(data_source_file,file_ext)\n",
    "    #if valid data source exist in the given data source -\n",
    "    if address_data.empty==False:\n",
    "        #extract the required fields -\n",
    "        # use std naming for address data attributes - across all data sources\n",
    "        # address_string, pincode, city, state\n",
    "        # use promise data for mapping - pincode - to city, state\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "\n",
    "def isEnglish(s):\n",
    "    return s.isascii()\n",
    "\n",
    "def get_soundex(name):\n",
    "        \"\"\"Get the soundex code for the string\"\"\"\n",
    "        name = name.upper()\n",
    "\n",
    "        soundex = \"\"\n",
    "        soundex += name[0]\n",
    "\n",
    "        dictionary = {\"BFPV\": \"1\", \"CGJKQSXZ\":\"2\", \"DT\":\"3\", \"L\":\"4\", \"MN\":\"5\", \"R\":\"6\", \"AEIOUHWY\":\".\"}\n",
    "\n",
    "        for char in name[1:]:\n",
    "            for key in dictionary.keys():\n",
    "                if char in key:\n",
    "                    code = dictionary[key]\n",
    "                    if code != soundex[-1]:\n",
    "                        soundex += code\n",
    "\n",
    "        soundex = soundex.replace(\".\", \"\")\n",
    "        soundex = soundex[:4].ljust(4, \"0\")\n",
    "\n",
    "        return soundex\n",
    "\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    #standard stemming, lemmatization - should be avoided in case of address data - as the exact word is required\n",
    "    #remove special characters, and convert address string to lowercase\n",
    "    # x = re.search(\"^A-Za-z0-9\", txt)\n",
    "    ptext=''\n",
    "    #remove non-ascii characters\n",
    "    for x in text:\n",
    "        if isEnglish(x)==True:\n",
    "            ptext=ptext+x\n",
    "\n",
    "    preprocessed_text=ptext\n",
    "    ptext=''\n",
    "\n",
    "    for x in preprocessed_text:\n",
    "        if x.isalnum()==True:\n",
    "            ptext=ptext+x\n",
    "        #replace special characters with space\n",
    "        elif x.isalnum()==False:\n",
    "            ptext=ptext+' '\n",
    "\n",
    "    preprocessed_text=ptext\n",
    "    #convert string to lower\n",
    "    preprocessed_text=preprocessed_text.lower()\n",
    "\n",
    "    #spelling correction - for city, state,\n",
    "\n",
    "\n",
    "    #phonetic similarity\n",
    "def soundex_similarity(text,text_list):\n",
    "    from pyphonetics import Soundex\n",
    "    soundex = Soundex()\n",
    "    for t in text_list:\n",
    "        replace_text_with=t\n",
    "        print(\"t: \",t)\n",
    "        print(\"t type: \",type(t))\n",
    "        text=text.encode('utf-8').decode('utf-8')\n",
    "        t=t.encode('utf-8').decode('utf-8')\n",
    "        text_similar=soundex.sounds_like(text,t)\n",
    "        # text_similarity_score=soundex.phonetics(text,t)\n",
    "        if text_similar:\n",
    "            replace_text_with=t\n",
    "            break\n",
    "    return replace_text_with\n",
    "def metaphone_similarity(text,text_list):\n",
    "    from pyphonetics import Metaphone\n",
    "    metaphone = Metaphone()\n",
    "    for t in text_list:\n",
    "        replace_text_with=t\n",
    "        text=text.encode('utf-8').decode('utf-8')\n",
    "        t=t.encode('utf-8').decode('utf-8')\n",
    "        text_similar=metaphone.sounds_like(text,t)\n",
    "        # text_similarity_score=metaphone.phonetics(text,t)\n",
    "        if text_similar:\n",
    "            replace_text_with=t\n",
    "            break\n",
    "    return replace_text_with\n",
    "\n",
    "def refined_soundex_similarity(text,text_list):\n",
    "    from pyphonetics import RefinedSoundex\n",
    "    refined_soundex = RefinedSoundex()\n",
    "    for t in text_list:\n",
    "        replace_text_with=t\n",
    "        text=text.encode('utf-8').decode('utf-8')\n",
    "        t=t.encode('utf-8').decode('utf-8')\n",
    "        text_similar=refined_soundex.sounds_like(text,t)\n",
    "        # text_similarity_score=refined_soundex.phonetics(text,t)\n",
    "\n",
    "        if text_similar:\n",
    "            replace_text_with=t\n",
    "            break\n",
    "    return replace_text_with\n",
    "\n",
    "def fuzzy_soundex_similarity(text,text_list):\n",
    "    from pyphonetics import FuzzySoundex\n",
    "    fuzzy_soundex = FuzzySoundex()\n",
    "    for t in text_list:\n",
    "        replace_text_with=t\n",
    "        text=text.encode('utf-8').decode('utf-8')\n",
    "        t=t.encode('utf-8').decode('utf-8')\n",
    "        text_similar=fuzzy_soundex.sounds_like(text,t)\n",
    "        # text_similarity_score=fuzzy_soundex.phonetics(text,t)\n",
    "\n",
    "        if text_similar:\n",
    "            replace_text_with=t\n",
    "            break\n",
    "    return replace_text_with\n",
    "\n",
    "def levenshtein_similarity(text,text_list):\n",
    "    from pyphonetics import RefinedSoundex\n",
    "    levenshtein_similarity = RefinedSoundex()\n",
    "    for t in text_list:\n",
    "        replace_text_with=t\n",
    "        text=text.encode('utf-8').decode('utf-8')\n",
    "        print(\"t: \",t)\n",
    "        print(\"t type: \",type(t))\n",
    "        t=t.encode('utf-8').decode('utf-8')\n",
    "        text_similar=levenshtein_similarity.distance(text,t,metric='levenshtein')\n",
    "        # text_similarity_score=levenshtein_similarity.phonetics(text,t)\n",
    "        if text_similar==0:\n",
    "            replace_text_with=t\n",
    "            break\n",
    "    return replace_text_with\n",
    "\n",
    "\n",
    "def hamming_similarity(text,text_list):\n",
    "    from pyphonetics import RefinedSoundex\n",
    "    levenshtein_similarity = RefinedSoundex()\n",
    "\n",
    "    for t in text_list:\n",
    "        replace_text_with=t\n",
    "        text=text.encode('utf-8').decode('utf-8')\n",
    "        t=t.encode('utf-8').decode('utf-8')\n",
    "        text_similar=levenshtein_similarity.distance(text,t,metric='hamming')\n",
    "        # text_similarity_score=levenshtein_similarity.phonetics(text,t)\n",
    "        if text_similar==0:\n",
    "            replace_text_with=t\n",
    "            break\n",
    "    return replace_text_with\n",
    "def phonetic_similarity_city_and_state(city, state):\n",
    "    #get the similarity score using each type of phonetic similarity matching\n",
    "    #using soundex algorithm\n",
    "    promise_data=pd.read_parquet('promise_state_name.pq')\n",
    "    #convert all strings in the df - to lower\n",
    "    promise_data['city']=promise_data['city'].apply(lambda x: str.lower(x))\n",
    "    promise_data['stateName']=promise_data['stateName'].apply(lambda x: str.lower(x))\n",
    "    city_list=list(promise_data.loc[promise_data['city'].isnull()==False,'city'].drop_duplicates().to_numpy())\n",
    "    state_list=list(promise_data.loc[promise_data['stateName'].isnull()==False,'stateName'].drop_duplicates().to_numpy())\n",
    "    replace_state_name_with=state\n",
    "    replace_city_name_with=city\n",
    "    if replace_state_name_with not in state_list:\n",
    "        # try mapping a state name - using soundex similarity\n",
    "            replace_state_name_with=levenshtein_similarity(state,state_list)\n",
    "            if replace_state_name_with not in state_list:\n",
    "                replace_state_name_with=hamming_similarity(state,state_list)\n",
    "                if replace_state_name_with not in state_list:\n",
    "                    replace_state_name_with=soundex_similarity(state,state_list)\n",
    "                    if replace_state_name_with not in state_list:\n",
    "                        replace_state_name_with=metaphone_similarity(state,state_list)\n",
    "                        if replace_state_name_with not in state_list:\n",
    "                            replace_state_name_with=refined_soundex_similarity(state,state_list)\n",
    "                            if replace_state_name_with not in state_list:\n",
    "                                replace_state_name_with=fuzzy_soundex_similarity(state,state_list)\n",
    "                                if replace_state_name_with not in state_list:\n",
    "                                                print(\"no exact state name match found....\")\n",
    "\n",
    "    if replace_city_name_with not in city_list:\n",
    "        # try mapping a city name - using soundex similarity\n",
    "\n",
    "            replace_city_name_with=levenshtein_similarity(city,city_list)\n",
    "            if replace_city_name_with not in city_list:\n",
    "                replace_city_name_with=hamming_similarity(city,city_list)\n",
    "                if replace_city_name_with not in city_list:\n",
    "                    replace_city_name_with=soundex_similarity(city,city_list)\n",
    "                    if replace_city_name_with not in city_list:\n",
    "                        replace_city_name_with=metaphone_similarity(city,city_list)\n",
    "                        if replace_city_name_with not in city_list:\n",
    "                            replace_city_name_with=refined_soundex_similarity(city,city_list)\n",
    "                            if replace_city_name_with not in city_list:\n",
    "                                replace_city_name_with=fuzzy_soundex_similarity(city,city_list)\n",
    "                                if replace_city_name_with not in city_list:\n",
    "                                    replace_city_name_with=levenshtein_similarity(city,city_list)\n",
    "                                    if replace_city_name_with not in city_list:\n",
    "                                        replace_city_name_with=hamming_similarity(city,city_list)\n",
    "                                        if replace_city_name_with not in city_list:\n",
    "                                            print(\"no exact city name match found....\")\n",
    "    return replace_city_name_with, replace_state_name_with\n",
    "\n",
    "# def spell_corrector\n",
    "def translate_to_english(text):\n",
    "\n",
    "    translator = google_translator()\n",
    "\n",
    "    sentence = text\n",
    "    translate_text = translator.translate(sentence,lang_tgt='en')\n",
    "    return translate_text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
